{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.environ['ITHEMAL_HOME'], 'learning', 'pytorch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import data.data_cost as dt\n",
    "import common_libs.utilities as ut\n",
    "import models.train as tr\n",
    "import models.graph_models as md\n",
    "import models.losses as ls\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dt.load_dataset('/home/ithemal/ithemal/learning/pytorch/inputs/embeddings/code_delim.emb', data_savefile='/home/ithemal/ithemal/learning/pytorch/saved/time_skylake_1217.data')\n",
    "\n",
    "ys = np.array([d.y for d in data.train])\n",
    "mean = ys.mean()\n",
    "std = ys.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = data.final_embeddings.shape[1]\n",
    "model = md.GraphNN(embedding_size, 256, 1, False)\n",
    "model.set_learnable_embedding(mode = 'none', dictsize = max(data.word2id) + 1, seed = data.final_embeddings)\n",
    "\n",
    "loss_ema = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = torch.load('../saved/edges_none_01-05-19_22:26:09.mdl')\n",
    "# model.load_state_dict(state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "def pred_loss(item, train=True, backward=True):\n",
    "    pred = model(item)\n",
    "    model.remove_refs(item)\n",
    "    y = np.log(item.y)\n",
    "    loss = torch.sqrt(loss_fn(pred, torch.FloatTensor([y]).squeeze())) / y\n",
    "    if backward:\n",
    "        loss.backward()\n",
    "    return pred.item(), loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "maxnorm = lambda vecs: max((p.data.norm(2).item() for p in vecs))\n",
    "\n",
    "while True:\n",
    "    adam.zero_grad()\n",
    "    item = random.choice(data.train)\n",
    "    pred, loss = pred_loss(item)\n",
    "    adam.step()\n",
    "\n",
    "    loss_ema = 0.999 * loss_ema + 0.001 * loss\n",
    "    max_param = maxnorm(model.parameters())\n",
    "    max_grad = maxnorm(filter(lambda p: p is not None, map(lambda p: p.grad, model.parameters())))\n",
    "\n",
    "    report = 'EMA: {:.2f}, Loss: {:.3f}, Max param: {:.2f}, Max Grad: {:.2f} ({:.2f}, {:.2f})'.format(\n",
    "        loss_ema, loss, max_param, max_grad, pred, np.log(item.y))\n",
    "    print(report + ' '*20, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.Train(model, data, 4)\n",
    "train.loss_fn = ls.mse_loss\n",
    "train.print_fn = train.print_final\n",
    "train.correct_fn = train.correct_regression\n",
    "train.num_losses = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, predicted = train.validate('/tmp/foo')\n",
    "actual = np.array(actual)\n",
    "predicted = np.exp(np.array(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_t = torch.Tensor(actual)\n",
    "predicted_t = torch.Tensor(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(actual - predicted) / actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (a, p) in zip(actual_t, predicted_t):\n",
    "    train.correct_regression(a, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.correct / float(len(data.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lstm_token.weight_ih_l0.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(len(ut.get_sym_dict()[0]))\n",
    "\n",
    "class AlexDagRnn(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(AlexDagRnn, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        sym_dict, _ = ut.get_sym_dict()\n",
    "        self.max_tok = len(sym_dict) + 1\n",
    "        \n",
    "        embedding = nn.Embedding(self.max_tok, self.embedding_size)\n",
    "        initrange = 0.5 / self.embedding_size\n",
    "        embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.final_embeddings = embedding\n",
    "        \n",
    "        self.linear_token_1 = nn.Linear(self.embedding_size, self.hidden_size)\n",
    "        self.linear_token_2 = nn.Linear(3 * self.hidden_size, self.hidden_size)\n",
    "        self.lstm_ins = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def embed_instr(self, instr):\n",
    "        op = self.final_embeddings(instr.opcode)\n",
    "        srcs = [self.final_embeddings(min(src, self.max_tok)) for src in instr.srcs] + [torch.zeros(self.embedding_size)]\n",
    "        \n",
    "    def forward(self, datum):\n",
    "        embeddings = {}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
