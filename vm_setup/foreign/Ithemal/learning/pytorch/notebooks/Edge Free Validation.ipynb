{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (ignore all this stuff, scroll down to Interpretability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.environ['ITHEMAL_HOME'], 'learning', 'pytorch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import data.data_cost as dt\n",
    "import common_libs.utilities as ut\n",
    "import models.train as tr\n",
    "import models.graph_models as md\n",
    "import models.losses as ls\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dt.load_dataset('/home/ithemal/ithemal/learning/pytorch/inputs/embeddings/code_delim.emb', data_savefile='/home/ithemal/ithemal/learning/pytorch/saved/time_skylake_1217.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data.data:\n",
    "    item.block.remove_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = data.final_embeddings.shape[1]\n",
    "model = md.GraphNN(embedding_size, 256, 1, False)\n",
    "model.set_learnable_embedding(mode = 'none', dictsize = max(data.word2id) + 1, seed = data.final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.Train(model, data, 4)\n",
    "train.loss_fn = ls.mse_loss\n",
    "train.print_fn = train.print_final\n",
    "train.correct_fn = train.correct_regression\n",
    "train.num_losses = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../saved/edges_none_01-07-19_06:42:33.mdl')\n",
    "model.load_state_dict(state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(item):\n",
    "    model.remove_refs(item)\n",
    "    model.init_bblstm(item)\n",
    "    roots = item.block.find_roots()\n",
    "\n",
    "    root_hidden = []\n",
    "    for instr in roots:\n",
    "        token_embeds_lstm = torch.FloatTensor(instr.tokens).unsqueeze(1)\n",
    "        _, (ins_embed, _) = model.lstm_token(token_embeds_lstm, model.init_hidden())\n",
    "        _, (ins_hidden, _) = model.lstm_ins(ins_embed, model.init_hidden())\n",
    "        root_hidden.append(ins_hidden.squeeze())\n",
    "\n",
    "    final_hidden = root_hidden[0]\n",
    "    for hidden in root_hidden[1:]:\n",
    "        final_hidden = model.reduction(final_hidden,hidden)\n",
    "    pred = model.linear(final_hidden).squeeze()\n",
    "    model.remove_refs(item)\n",
    "    \n",
    "    return pred, root_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.correct = 0\n",
    "total_loss = 0\n",
    "\n",
    "for datum in data.test:\n",
    "    pred, _ = predict(datum)\n",
    "    y = torch.FloatTensor([datum.y]).squeeze()\n",
    "    total_loss += ls.mse_loss(pred, y)[0].item()\n",
    "    train.correct_regression(pred, y)\n",
    "\n",
    "print('Test loss: {:.2f}'.format(total_loss / float(len(data.test))))\n",
    "print('Test accuracy: {:.2f}'.format(train.correct / float(len(data.test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability\n",
    "\n",
    "Since this is a simple model (LSTM cell on each instruction embedding, `max` to combine instruction embeddings, then a linear layer), we can get some interpretable results:\n",
    "- `desire`, the prediction of the given instruction if it were the only instruction in the block\n",
    "- `weight`, the percentage of the (absolute value) weight in the linear layer that is multiplied by each slot in the final vector where the instruction won the `max`\n",
    "- `total contrib`, the actual contribution of the instruction to the final prediction (the slots where it won `max` times the weights of those slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = data.test[21]\n",
    "\n",
    "pred, root_hidden = predict(item)\n",
    "\n",
    "total_weight = model.linear.weight.data.squeeze().abs().sum()\n",
    "\n",
    "for i in range(len(root_hidden)):\n",
    "    max_vals, max_idxs = torch.stack(root_hidden).max(dim=0)\n",
    "    idxs = np.where(max_idxs.data.numpy() == i)\n",
    "    i_vals = max_vals[idxs]\n",
    "    i_weights = model.linear.weight.data.squeeze()[idxs]\n",
    "    i_weight_perc = 100 * i_weights.abs().sum() / total_weight\n",
    "    i_w_contrib = (i_weights * i_vals).sum()\n",
    "    i_desire = (model.linear(root_hidden[i])).squeeze().sum().item()\n",
    "    print('{:<60}: desire {:6.2f}, weight {:2.0f}%, total contrib {:6.2f}'.format(\n",
    "        item.block.instrs[i],\n",
    "        i_desire,\n",
    "        i_weight_perc,\n",
    "        i_w_contrib,\n",
    "    ))\n",
    "\n",
    "print('\\npred: {:.2f}, actual: {:.2f}'.format(\n",
    "    pred.item(),\n",
    "    item.y,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the best we could do if we were to have a constant prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = data.train\n",
    "\n",
    "starts = [None] * len(dataset)\n",
    "ends = [None] * len(dataset)\n",
    "\n",
    "for i, datum in enumerate(dataset):\n",
    "    starts[i] = datum.y * 0.75\n",
    "    ends[i] = datum.y * 1.25\n",
    "\n",
    "starts.sort()\n",
    "ends.sort()\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "max_val = 0\n",
    "max_count = 0\n",
    "curr_count = 0\n",
    "for val, typ in sorted(itertools.chain(zip(starts, itertools.repeat('start')), zip(ends, itertools.repeat('end')))):\n",
    "    if typ == 'start':\n",
    "        curr_count += 1\n",
    "        if curr_count > max_count:\n",
    "            max_count = curr_count\n",
    "            max_val = val\n",
    "    elif typ == 'end':\n",
    "        curr_count -= 1\n",
    "    xs.append(val)\n",
    "    ys.append(curr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.xlabel('Time prediction')\n",
    "plt.ylabel('#Correct on prediction')\n",
    "plt.title('Best single prediction (learned from train)')\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_prediction = xs[np.argmax(ys)]\n",
    "actual = np.array([datum.y for datum in data.test])\n",
    "single_prediction = np.ones_like(actual) * m_prediction + 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = np.sum(np.abs(actual - single_prediction) / (single_prediction + 1e-3) * 100 < 25)\n",
    "print('Correct: {}'.format(n_correct / float(len(data.test))))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
